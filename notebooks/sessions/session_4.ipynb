{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 4: Modulating internal states and Sensing other agent's attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last practical session, we saw how to run multiple behaviors in parallel on multiple agents. In this section we will see more advanced methods for combining behaviors by modulating their activations according to internal states of the agent and by allowing them to sense the attributes of others. In order to start with a clean basis, let's first provide the definition of several behaviors. These four behaviors are simply implementations of the [Braitenberg vehicles](https://docs.google.com/presentation/d/1s6ibk_ACiJb9CERJ_8L_b4KFu9d04ZG_htUbb_YSYT4/edit#slide=id.g31e1b425a3_0_0) we have seen in class, where the `sensed_entities` (i.e. what is sensed by the proximeters) are other `agents`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fear(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"agents\"])\n",
    "    left_wheel = left\n",
    "    right_wheel = right\n",
    "    return left_wheel, right_wheel\n",
    "\n",
    "def aggression(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"agents\"])\n",
    "    left_wheel = right\n",
    "    right_wheel = left\n",
    "    return left_wheel, right_wheel\n",
    "\n",
    "def love_cuddly(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"agents\"])\n",
    "    left_wheel = 1 - left\n",
    "    right_wheel = 1 - right   \n",
    "    return left_wheel, right_wheel\n",
    "\n",
    "def love_shy(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"agents\"])\n",
    "    left_wheel = 1 - right\n",
    "    right_wheel = 1 - left   \n",
    "    return left_wheel, right_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the arguments of the behavior functions above, `agent` is arbitrary name. We choose here to call it `agent` in order to avoid a confusion when dealing with multiple agents. The only important thing is that you use the same name in the argument and in the function body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a fifth behavior for obstacle avoidance, where obstacles are objects little and big squares, named `s_obstacles` and `b_obstacles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obstacle_avoidance(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"s_obstacles\", \"b_obstacles\"])\n",
    "    left_wheel = 1 - right\n",
    "    right_wheel = 1 - left   \n",
    "    return left_wheel, right_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:** First import the functions `start_server_and_interface` and `stop_server_and_interface`, and the `NotebookController` class. Start the server and interface for this session, and create a controller object to interact with the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vivarium.controllers.notebook_controller import NotebookController\n",
    "from vivarium.utils.handle_server_interface import start_server_and_interface, stop_server_and_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_server_and_interface(scene_name=\"session_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = NotebookController()\n",
    "controller.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emergent behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running one or several behaviors on a agent, you might observe behavioral responses that are not present in any of the behavior definitions. This usually doesn't mean that there is a bug, it could just be the result of an emergent behavioral property. Let's analyze this on a quick example you may have already seen in session 3, by running the `obstacle_avoidance` behavior on each agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach the behavior avoidance to all agents and check the behaviors\n",
    "for agent in controller.agents:\n",
    "    agent.attach_behavior(obstacle_avoidance)\n",
    "    agent.print_behaviors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session's environment contains more entities than previous ones, which might slow down the simulation. To speed it up, you can increase the number of steps the simulation performs on the server for each step done by the controller. To do this, open the `SIMULATOR` panel in the web interface and find the `num_steps_lax` parameter. It is set to 6 by default; you can increase it to 20 to see if the simulation runs faster. Make sure to use this mechanism wisely, as increasing this number too much might lead to a simulation that is less reactive to your commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** You will observe that the 3 agents of the scene seem to be attracted to objects that are not handled by the behavior (e.g. other agents or resources). Analyse this phenomena and explain below why it is occuring in a few lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click on this cell to enter your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing agents interactions\n",
    "\n",
    "First, let's detach all the agent's behaviors and stop their motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    agent.detach_all_behaviors(stop_motors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now analyze the interaction between two agents that are equipped with different behaviors. First assign a variable to all agents of the simulation so we can easily access them indivually in the next steps. As we saw in previous sessions, the `controller.agents` is a list, and we can access its elements with Python indexing system (0 corresponding to the first element, 1 to the second etc). We will only consider the first the agents (index 0 and 1) for this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_0 = controller.agents[0]\n",
    "agent_1 = controller.agents[1]\n",
    "agent_2 = controller.agents[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To differentiate the agents, we will also assign them different colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_0.color = \"blue\"\n",
    "agent_1.color = \"cyan\"\n",
    "agent_2.color = \"black\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** In addition to the `obstacle_avoidance` behavior, run several combinations of the four other behaviors defined above. For example, run `obstacle_avoidance` and `fear` on `agent_0` ; and `obstacle_avoidance` and `aggression` on `agent_1`. Keep the `agent_2` still for the moment. Find two of these combinations that you consider as interesting (e.g. because they result in a relatively complex interaction pattern, or because they can be linked to animal behavior) and describe them in a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the previous sesssion and the last exercice, it is possible to run several behaviors in parallel on the same agent. When doing it, the motor activation sent to each wheel corresponds to the average of the motor activation returned by each behavior (this averaging is implemented internally, you don't need to worry about it). \n",
    "\n",
    "It is also possible to specify the weight of each running behavior, i.e. how much it will count in the averaging. This is done by returning three values in the function defining a behavior (instead of two as we were doing until now: one for the left wheel activation and one for the right one). For example, if we want to run the `obstacle_avoidance` behavior with a weight of 1 and the `fear` behavior with a weight of 0.5 on `agent_2`, we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2.attach_behavior(obstacle_avoidance, weight=1)\n",
    "agent_2.attach_behavior(fear, weight=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be hard to effectively see the weights in action for these two behaviors as they both lead the agent to avoid other entities. In order to better visualize the effect of the weights, we will define two new behaviors with opposite effects: `aggress_all` and `avoid_all`. The `aggress_all` behavior will make the agent move towards the sensed entities, while the `avoid_all` behavior will make it move away from them. We will add a bigger weight to the `aggress_all` behavior than to the `avoid_all` behavior, and observe the agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggress_all(agent):\n",
    "    left, right = agent.sensors()\n",
    "    left_wheel = right\n",
    "    right_wheel = left  \n",
    "    return left_wheel, right_wheel\n",
    "\n",
    "def avoid_all(agent):\n",
    "    left, right = agent.sensors()\n",
    "    left_wheel = 1 - right\n",
    "    right_wheel = 1 - left   \n",
    "    return left_wheel, right_wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2.detach_all_behaviors()\n",
    "agent_2.attach_behavior(aggress_all, weight=1)\n",
    "agent_2.attach_behavior(avoid_all, weight=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** What are the effects of the weights on the agent's behavior? Why does it keep moving even when it doesn't sense anything ? Describe the observed behavior in a few lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this, the wheel activations returned by the `agress_all` behavior will have more weight than those returned by the `avoid_all` behavior. For example, if `avoid_all` returns 0.6 for the left wheel, and `agress_all` returns 0.9, then the total activation of that wheel will be $(0.6 * 0.2 + 0.9 * 1) / (0.2 + 1) = 0.85$ (i.e. the average of both values weighted by their respective activation). Note that when no weight is provided in a behavior definition (i.e. when the behavior function returns only two values as usual), the corresponding behavior is set with a default weight of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting behaviors according to internal states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weighting is particularly useful to activate a behavior according to some internal states of the agent. Let's consider a agent that eats resourcess (as in the previous session) and that eating those resourcess allows to raise a simulated energy level of the agent. We want to continuously compute the energy level of the agent according to how much resourcess it has recently eaten. To do so, we first need a way to know when a agent has eaten for the last time. There are several ways of doing this that can serve in different use cases:\n",
    "\n",
    "- agent.has_eaten(): function that tells if the agent has eaten since the last call to this function, defaults to False\n",
    "- agent.time_since_feeding: attribute that tells the time since the last meal, defaults to infinity when the agent has never eaten\n",
    "- agent.has_eaten_since(t): function that tells if the agent has eaten since time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"agent 0: -ate: {agent_0.has_eaten()} -time since feeding: {agent_0.time_since_feeding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see than when agents haven't started eating, the time since feeding is infinite (`inf`), and the `has_eaten` function returns False. When they start eating, the time since feeding is reset to 0, and the `has_eaten` function returns True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check this for all agents of the scene at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    print(f\"agent {agent.idx}: -ate: {agent.has_eaten()} -time since feeding: {agent.time_since_feeding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this more interesting, let's start the eating mechanism and the spawning of resources as in session 3. Then we will use the same cell as before to check how `time_since_feeding` and `has_eaten` evolve for all agents. Additionally, attach both the `obstacle avoidance` and the `foraging` behavior on the agents, and execute the cell above at different times to understand correctly how the `has_eaten`function and `time_since_deefing` attribute work. Remember we have already defined a `foraging_behavior` in the last session, which can be implemented like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foraging(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"resources\"])\n",
    "    left_activation = right\n",
    "    right_activation = left\n",
    "    return left_activation, right_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** Remove the current behaviors of all the agents and replace them with `obstacle_avoidance` and `foraging`. Then, make sure they are correctly attached with the `print_behaviors` function of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's verify the subtypes list of the controller and the FPS of the simulation loop, in order to choose our parameters for the `start_resources_apparition` function of the controller and the `diet` of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Subtypes in the environment:\")\n",
    "controller.print_subtypes_list()\n",
    "\n",
    "print(\"\\nFPS of the controller:\")\n",
    "controller.print_fps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, add `resources` to the diet of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add resources to the agents diet\n",
    "for agent in controller.agents:\n",
    "    agent.diet = [\"resources\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, start the resources apparition and eating mechanisms. Choose your resources spawning execution interval apparition according to the FPS of the simulation loop. For example, if the FPS is 20, you can set the interval to 100 to spawn resources every 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start resources apparition and eating mechanism\n",
    "controller.start_resources_apparition(interval=50)\n",
    "controller.start_eating_mechanism(interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now observe the evolution of the `time_since_feeding` and `has_eaten` attributes of all agents. You can execute the cell below several times to see how the attributes evolve over time. Try to understand how they both work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    print(f\"agent {agent.idx}: -ate: {agent.has_eaten()} -time since feeding: {agent.time_since_feeding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to continuously compute the energy level of a agent, so that the level decreases slowly when nothing is eaten and increases whenever food is consumed. This can be done by attaching and starting a *routine* to a agent. The definition of a routine is very similar to the definition of a behavior, except that it doesn't return any value (whereas a behavior always returns the left and right wheel activations, and optionally a weight). Thus, a routine corresponds to a set of instructions that are executed at a particular interval (as in a behavior), e.g. to compute some agent's internal states according to its interaction with the environment.\n",
    "\n",
    "Let's define a routine called `foraging_drive` that computes the energy level as specified above. In order to do so, we will first define a few parameters, such as the initial and the maximum level of energy of agents. Then, we will attach an `energy_level` attribute to each agent\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_energy_level = 100.\n",
    "init_energy_level = 50.\n",
    "\n",
    "for agent in controller.agents:\n",
    "    agent.energy_level = init_energy_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we can access this attribute for all agents in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    print(f\"Agent {agent.idx} energy level is: {agent.energy_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you execute the cell above several times, note that the `energy_level` attribute of the agents is static because we didn't do anything to update it yet ! Let's change this by defining the `foraging_drive` routine as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foraging_drive(agent): \n",
    "    if agent.has_eaten():\n",
    "        # if the agent has eaten a resources, increase its energy level\n",
    "        agent.energy_level += 10  \n",
    "        # also increase the agent's diameter, and its eating range proportionally (otherwise agent cannot eat when ti is too big)\n",
    "        agent.diameter += 1.\n",
    "        agent.eating_range += 1.\n",
    "    else:\n",
    "        # decrease energy level\n",
    "        agent.energy_level -= 0.01  # otherwise (nothing eaten), decrease the energy level by 0.01\n",
    "    # The line below bounds the value of the energy level between 0 and max_energy_level\n",
    "    agent.energy_level = min(max_energy_level, max(agent.energy_level, 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As for a behavior, the function defining a routine takes a `agent` as an argument representing the agent on which the routine is attached. Attaching a routine is done with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign foraging_drive to agent_0 and decrease its size to recognize it more easily\n",
    "agent_0.diameter = 7\n",
    "agent_0.attach_routine(foraging_drive, interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before starting a routine where we update the attribute of an agent, we need to make sure this attribute exists. This is why we defined the `energy_level` attribute before defining the routine.\n",
    "\n",
    "As for a behavior, a routine is attached to an agent at a given interval. The code of the `foraging_drive` function above will be executed every `interval` steps, consequently modulating the agent energy level stored in `agent.energy_level` according to how good the agent is at catching resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_0.energy_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the evolution of the energy level in `agent_0` by executing the cell below several times. What happens when he eats a resource ? What happens when he doesn't eat for a while ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also remove the routine from an agent similarly to a behavior with the `detach_routine` and `detach_all_routines` functions, and check them with the `print_routines` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    agent.print_routines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_0.detach_all_routines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Modify the `foraging` behavior so that it is weighted according to the energy level of the agent: the lower the energy level, the higher `foraging` is weighted. However, as seen in Q1 above, the agent might still be attracted to resources even when the `foraging` weight is null. Solve this issue by adding another behavior that drives the agent away from resources according to the energy level: the higher the energy level, the more the agent is repulsed from resourcess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now detach all the behaviors and the routines before going to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    agent.detach_all_routines()\n",
    "    agent.detach_all_behaviors(stop_motors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensing other agent's attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to allow a agent to sense attributes from other agents. For example, one might want to define different species of agent (cats and mouses for example) so that how a agent interact with another depends on their respective species. To achieve this, each agent can be set with a specific attribute, e.g `agent.species = \"cat\"` and that attribute can be sensed by other agents whenever it is detected by a proximeter. Let's try it with three agents. \n",
    "\n",
    "We will define a \"cat\" `species` for `agent_0`, and make him bigger, and a \"mouse\" `species` for agents 1 and 2. We will also make them smaller and give them the same color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set attributes of agent 0\n",
    "agent_0.species = \"cat\"\n",
    "agent_0.diameter = 12.\n",
    "\n",
    "# set the same attributes for agent 1 and agent 2\n",
    "agent_1.species = agent_2.species = \"mouse\"\n",
    "agent_1.diameter = agent_2.diameter = 7.\n",
    "agent_1.color = agent_2.color = \"cyan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have one cat and two mouses. Place the agents (either with the interface or with code) so that `agent_2` (mouse) is sensing `agent_0` (cat) on its left proximeter and `agent_1` (mouse) on its right proximeter. You can set the position of an entity by using its `x_position` and `y_position` attributes. You can also do it with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First place the agent 2 at a specific location, and make it face the top of the environment\n",
    "agent_2.x_position = 150\n",
    "agent_2.y_position = 150\n",
    "agent_2.orientation = 3.1415 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, place the two other agents accordingly so they are sensed on the left and right of agent 2\n",
    "agent_0.x_position = 130\n",
    "agent_0.y_position = 170\n",
    "\n",
    "agent_1.x_position = 170\n",
    "agent_1.y_position = 170"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just make sure there are no objects sensed between `agent_2` (at the bottom) and the two other agents. You can manually move the objects if it is the case. The center of your scene should look roughly like this image:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Then, to return the sensed entities by left and right proximeters, we can use the `sensed_entities` function of the agents. This returns the left and right entities if they are sensed, and `None` otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_left, ent_right = agent_2.sensed_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what are the entities detected by a agent by running the following cell. Let's analyze it with more details. First, we print infos for the left entity. We start by ensuring it exists with ``Ã¬f ent_left:``, because it is possible there are no entity sensed at the left of our agent, and this value would therefore be `None`. Then if the entity exists, we print its infos with the `print_infos()` function, else we just print that there are no entity sensed on this side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the infos of the sensed entities\n",
    "print(\"Left entity:\")\n",
    "if ent_left:\n",
    "    ent_left.print_infos()\n",
    "else:\n",
    "    print(\"No entity sensed\")\n",
    "\n",
    "print(\"\\nRight entity:\")\n",
    "if ent_right:\n",
    "    ent_right.print_infos()\n",
    "\n",
    "else:\n",
    "    print(\"No entity sensed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also directly sense specific attributes of these sensed entities with the `sense_attributes` function. Make sure to specify the `sensed_attribute` you want to sense (e.g 'diameter', 'species', etc.). You can also specify the `default_value` value to return if the attribute is not found (otherwise it is set to None). The function will return the given attribute of the left and right sensed entities if it exists (it can be either if the entity doesn't exist, or because the entity doesn't possess the specific attribute), and the default value otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, each entity has a diameter, so if you see a `None` value, it means that the entity isn't sensed by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_attr, r_attr = agent_2.sense_attributes(sensed_attribute=\"diameter\")\n",
    "print(f\"Left: {l_attr} Right: {r_attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this other case, we only defined species for the agents. We set the default value to 'unknown', so if an entity isn't detected, or doesn't have the attribute, it will return 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_attr, r_attr = agent_2.sense_attributes(sensed_attribute=\"species\", default_value=\"unknown\")\n",
    "print(f\"Left: {l_attr} Right: {r_attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sense_attributes` function takes 2 arguments: the name of the attribute we are interested in (here `\"species\"`) and the default value to return if a proximeter is not sensing any entity (here `none`). Note that this function works with any type of sensed entities, that is to say agents and any kind of objects.\n",
    "\n",
    "Move the agent 2 in the scene by activating its behaviors, and check how the attribute sensing is changing accordingly by re-executing the two above cells. Then try with other attributes, for example by setting an `age` attribute to the agents and sensing it from the proximeters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now extend the `fear` behavior so that mouses are only afraid by the cat but not by the other mouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fear_cat(agent):\n",
    "    (left, right) = agent.sensors(sensed_entities=[\"agents\"])\n",
    "    left_species, right_species = agent.sense_attributes(sensed_attribute=\"species\", default_value =\"none\")\n",
    "    left_activation = left if left_species == \"cat\" else 0\n",
    "    right_activation = right if right_species == \"cat\" else 0\n",
    "    return left_activation, right_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the obstacle avoidance on each agents to make them move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    agent.attach_behavior(obstacle_avoidance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run this behavior together with the `obstacle_avoidance` one on `agent_2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2.attach_behavior(fear_cat)\n",
    "agent_2.print_behaviors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check in the simulator that `agent_2` (mouse) is now avoiding `agent_0` (cat) but not `agent_1` (mouse). We can modify the `species` attribute of agents on the fly. Let's swap the species of `agent_0` and `agent_1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent_0.species = \"mouse\"\n",
    "agent_0.diameter = 7.\n",
    "agent_0.color = \"green\"\n",
    "\n",
    "agent_1.species = \"cat\"\n",
    "agent_1.diameter = 12.\n",
    "agent_1.color = \"blue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check in the simulator that `agent_2` (cyan mouse) is now avoiding `agent_1` (cat) but not `agent_0` (green mouse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Use the new functionalities we have seen in this session to design the following system:\n",
    "Two agents are equipped with behaviors to avoid obstacle and catch resourcess, as well as being attracted or repulsed by the other agent. Attraction and repulsion depend on the energy level of the other agent: the higher this level the more attraction, the lower this level the more repulsion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first detach all the behaviors\n",
    "for agent in controller.agents:\n",
    "    agent.detach_all_behaviors(stop_motors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**: Modify the previous simulation such that attraction and repulsion depend on how much a agent has been close to others in the recent past. To do so, define a `social_drive` routine that modulates the value of a `social_need` attribute in each agent. This social need continuously increases when other agents are far and decreases when a agent comes closer to its conspecifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_server_and_interface(safe_mode=False)\n",
    "controller.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you finished this session, you can either jump to :\n",
    "\n",
    "- [session 5](session_5_logging.ipynb) : How to log and plot data about the simulation \n",
    "- [session 6](session_6_bonus.ipynb) : Bonus session that explains how to use routines to a simple Eco-Evolutionary simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## About the mini-project\n",
    "\n",
    "The aim of the mini project is to show that you have understood all the concepts we have seen during these three practical sessions and that you can integrate them in a single demo. You can of course modify the V-REP scene at your will, e.g adding cups or trees. Don't forget to save you modified V-REP scene (`File -> Save scene as` in V-REP). You can start your project in a new Jupyter Notebook (`File -> New notebook -> Python3`) in the menu bar at the top of this notebook). Use both text cells (explaining what you are doing) and code cells (with the corresponding code to execute).\n",
    "\n",
    "Therefore, your project will consists of two files: a V-REP scene (extension `.ttt`) a Jupyter Notebook (extension `.ipynb`). Don't forget to save them, e.g. on a USB stick, before logging out from the UPF computers. See [this slide](https://docs.google.com/presentation/d/1FlAUyvNynYU4mDBE2o20pf2Bn5PC_9Id3SK8sCMfr-Q/edit#slide=id.g34900c6ead_0_0) for more information on the mini-project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
